# -*- coding: utf-8 -*-import numpy as npimport chainerimport chainer.functions as Fimport chainer.links as Limport chainer.initializers as Iimport chainerrlimport copyimport timeimport serialimport cv2import mathfn = 0class QFunction(chainer.Chain):    def __init__(self, obs_size, n_actions, n_hidden_channels=64):        super(QFunction, self).__init__()        with self.init_scope():            self.l1=L.Linear(obs_size, n_hidden_channels, initialW=I.HeNormal(scale=0.5))            self.l2=L.Linear(n_hidden_channels, n_hidden_channels, initialW=I.HeNormal(scale=0.5))            self.l3=L.Linear(n_hidden_channels, n_hidden_channels, initialW=I.HeNormal(scale=0.5))            self.l4=L.Linear(n_hidden_channels, n_actions, initialW=I.HeNormal(scale=0.5))    def __call__(self, x, test=False):        h = F.tanh(self.l1(x))        h = F.tanh(self.l2(h))        h = F.tanh(self.l3(h))        y = chainerrl.action_value.DiscreteActionValue(self.l4(h))        return ydef random_action():    return np.random.choice(range(5))gamma = 0.99alpha = 0.5max_number_of_steps = 1000#200  #1試行のstep数num_episodes = 1000  #総試行回数q_func = QFunction(3, 5)optimizer = chainer.optimizers.Adam(eps=1e-2)optimizer.setup(q_func)explorer = chainerrl.explorers.LinearDecayEpsilonGreedy(start_epsilon=1.0, end_epsilon=0.1, decay_steps=num_episodes, random_action_func=random_action)replay_buffer = chainerrl.replay_buffer.PrioritizedReplayBuffer(capacity=10 ** 6)phi = lambda x: x.astype(np.float32, copy=False)agent = chainerrl.agents.DoubleDQN(    q_func, optimizer, replay_buffer, gamma, explorer,    replay_start_size=500, update_interval=1, target_update_interval=100, phi=phi)width = 300  #Window のサイズ（横）height = 200  #Window のサイズ（縦）max_length = 100  #棒の半分の長さ（左右にmax_length 伸ばすため）dt = 0.02  #時間刻みgravity = 9.80665  #重力加速度r = 10.0  #ボールの半径theta_threshold_radians = 24 * math.pi / 360  #24度以上レールが傾いたら終了x_threshold = max_lengthfor episode in range(num_episodes):  #試行数分繰り返す    reward = 0.0    R = 0    done = False    s = 0.01    x = max_length-10    v = 0.0    ds = 0.0    for t in range(max_number_of_steps):  #1試行のループ        state = np.array((x,v,s))        action = agent.act_and_train(state, reward)        costheta = math.cos(s)        sintheta = math.sin(s)        v = v + 50*(gravity * sintheta) * dt        if x<-max_length and v<0:            v = 0.0        if x>max_length and v>0:            v = 0.0        x = x + v * dt        if episode%10==0:            k = cv2.waitKey(10)            if k==113:#q:                cv2.destroyAllWindows()                break            img = cv2.imread("300x200.bmp", flags=cv2.IMREAD_GRAYSCALE)            y = height/ 2 + x*math.tan(s)-r/costheta            x1 = width / 2 + max_length * costheta            y1 = height / 2 + max_length * sintheta            x2 = width / 2 - max_length * costheta            y2 = height / 2 - max_length * sintheta            cv2.rectangle(img, (int(width / 2-20), int(0)), (int(width / 2+20), int(height)), 198, -1)            cv2.circle(img, (int(x+width/2), int(y)), int(r), 127, -1)            cv2.line(img, (int(x1), int(y1)), (int(x2), int(y2)), 32, 4)            cv2.line(img, (int(0), int(height)), (int(t*width/max_number_of_steps), int(height)), 192, 20)            filename = 'Episode : '+ str(episode).zfill(4)            cv2.putText(img, filename, (0, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), thickness=1)            cv2.imshow("Simulation",img)        ds=0.0        if action == 0:            ds=0.01        elif action == 1:            ds=-0.01        elif action == 2:            ds=0.0        elif action == 3:            ds=-0.002        elif action == 4:            ds=+0.002        s = s + ds        done = s < -theta_threshold_radians or s > theta_threshold_radians        reward = 0.0        if not done:            if x>-20 and x<20:                reward = 1.0        else:            reward = 0.0            break                    R += reward    agent.stop_episode_and_train(state, reward, done)    print('Episode {}: step {}: reward {} done {}, statistics: {}, epsilon {}'.format(episode, t, R, done, agent.get_statistics(), agent.explorer.epsilon))agent.save('agent')